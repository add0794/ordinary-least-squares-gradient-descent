{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression: Ordinary Least Squares (OLS) vs. Gradient Descent (GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple regression is a statistical technique used to model the relationship between one dependent variable (or label) and two or more independent variables (or features). It is a powerful tool for understanding how changes in independent variables influence the dependent variable. While this modeling approach can be implemented using various strategies, I will focus on two main methods: \n",
    "- **Ordinary Least Squares (OLS)**, which derives the solution through pure mathematical analysis, \n",
    "- **Gradient Descent (GD)**, an iterative optimization algorithm often associated with machine learning.\n",
    "\n",
    "This article will derive the regression parameters using both OLS and GD, then compare their performance and applicability based on the following criteria:\n",
    "- **Computational Speed**: How quickly each method converges to a solution, particularly for datasets of varying sizes and dimensionalities.\n",
    "- **Memory Usage**: The amount of memory required by each approach, especially for large datasets where matrix operations (OLS) may become prohibitive.\n",
    "- **Convergence Behavior**: The robustness of GD to hyperparameter tuning (e.g., learning rate) and its ability to converge to an optimal solution in different conditions.\n",
    "- **Numerical Stability**: How each method performs when faced with ill-conditioned data or high multicollinearity among independent variables.\n",
    "- **Scalability**: Suitability for big data scenarios, where the efficiency of processing may differ significantly between the two methods.\n",
    "- **Ease of Implementation**: Practical considerations, including code complexity, ease of debugging, and availability of libraries or tools.\n",
    "- **Accuracy**: The ability of each method to minimize the error term (residuals) and produce reliable parameter estimates under different scenarios.\n",
    "\n",
    "Finally, the article will explore scenarios where one method is preferable over the other. For example:\n",
    "- OLS may excel in smaller datasets where computational resources are not constrained.\n",
    "- GD is often the method of choice for very large datasets or when working with models that extend beyond linear regression, such as neural networks.\n",
    "\n",
    "By analyzing these aspects, this article aims to provide a comprehensive comparison to help readers choose the appropriate method for their specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Deriving Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's focus on how the parameters are derived with OLS and GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use OLS\n",
    "\n",
    "#### 0) How to Get Started\n",
    "\n",
    "We start with the following equation:\n",
    "\n",
    "$$Xβ = y$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*X* := The design matrix of independent variables\\\n",
    "*β* := The vector of parameters (coefficients to be estimated)\\\n",
    "*y* := The vector of the dependent variable\n",
    "\n",
    "This is not possible, however, since there is always an error value, known as the *residuals*. \n",
    "\n",
    "Thus:\n",
    "\n",
    "$$X\\hat{β} + ε = \\hat{y}$$\n",
    "\n",
    "With that in mind, OLS can be derived through various approaches:\n",
    "\n",
    "#### 1) **Optimization using calculus**  \n",
    "\n",
    "In this scenario, we minimize the norm of $y - \\hat{y}$, which will give us the vector of parameters. $$\\min_β |y - \\hat{y}| = \\min_β |y - X\\hat{β}|^2$$\n",
    "\n",
    "I won't do the math for this, but if you're interested, it requires calculus and is a stronger solution since it follows along the maximum likelihood estimation (MLE).\n",
    "\n",
    "#### 2) **Using the Left Inverse**\n",
    "\n",
    "Because $Xβ$ is assumed to be full column rank, we can derive *β*.\n",
    "\n",
    "In other words, if we multiply $Xβ$ by its transpose, we can then multiply it by its inverse.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$X^TXβ = X^Ty$$\n",
    "$$(X^TX)^{-1}X^TXβ = (X^T X)^{-1}X^Ty$$\n",
    "$$Iβ = (X^TX)^{-1}X^Ty$$\n",
    "$$β = (X^T X)^{-1}X^Ty$$\n",
    "\n",
    "Note:\n",
    "*I* = The identity matrix\n",
    "\n",
    "Because β is predicting y, it is better to conclude that this derivation represents $\\hat{β}$.\n",
    "\n",
    "This is not the best approach because we are assuming $Xβ$ is full column rank, which may not be true if there is **Multicollinearity**.\n",
    "\n",
    "Why? If one parameter is a linear combination of any of the others, then the design matrix $X$ is not of full column rank (e.g. $β_3 = 4*β_1$).\n",
    "\n",
    "#### 3) Transforming $X$ to Row Canonical Form\n",
    "\n",
    "The equation\n",
    "\n",
    "$$X^TXβ = X^Ty$$\n",
    "\n",
    "can be row reduced to its canonical form to get the vector of parameters as well.\n",
    "\n",
    "This will make sense in the next derivation.\n",
    "\n",
    "#### **4) Using Orthogonal Projections**\n",
    "\n",
    "$Xβ = y$ can be seen as having a projection and an orthogonal component.\n",
    "\n",
    "If we view the parameters as predictions, then we have a projection:\n",
    "$$X\\hat{β} = \\hat{y}$$\n",
    "\n",
    "This is also known as $\\hat{y}$ projected onto the column space of $X\\hat{β}$\n",
    "\n",
    "The orthogonal component will be the nullspace $X\\hat{β}$, which is the residual: $y - \\hat{y}$ (or $y - X\\hat{β}$)\n",
    "\n",
    "In other words, the *residual* (orthogonal) and *prediction* (projection) add up to the vector of the dependent variable: y\n",
    "\n",
    "What now?\n",
    "\n",
    "Given that $y - X\\hat{β}$ is orthogonal to the column space of $X$, then it's orthogonal to each column of $X$:\n",
    "\n",
    "$$X^T*(y - X\\hat{β}) = 0$$\n",
    "$$X^Ty - X^TX\\hat{β} = 0$$\n",
    "$$X^Ty = X^TX\\hat{β}$$\n",
    "$$(X^TX)^{-1}X^Ty = \\hat{β}$$\n",
    "\n",
    "We could have solved for that using row reduction, where $X^TX$ is augmented by $X^Ty$, then put into row canonical form.\n",
    "\n",
    "#### **Conclusion**\n",
    "\n",
    "Regardless of which approach you use with OLS, the coefficients will be:\n",
    "$$\\hat{β} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### **0) How to Get Started**\n",
    "\n",
    "With GD, you must decide a cost function, AKA a loss or an objective function.\n",
    "\n",
    "For multiple regression, we start with the mean square error (MSE) as the cost function (it is most common):\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\beta_0 - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*N* := Number of data points\\\n",
    "*y_i* := The vector of the dependent variable\\\n",
    "*β_0* := The scalar of the independent variable's intercept\\\n",
    "*β_i* := The vector of parameters (coefficients to be estimated)\\\n",
    "*x_i* := The vector of independent variables\n",
    "\n",
    "Note: This has not been put in matrix form because GD uses calculus.\n",
    "\n",
    "At first, you might think, \"How can I calculate the residual without first having values for the parameters?\"\n",
    "\n",
    "GD iteratively determines the parameters' values.\n",
    "\n",
    "You add the value of the partial derivative of the MSE with regard to the particular parameter:\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\beta_0} = -\\frac{2}{N} \\sum_{i=1}^N (y_i - \\beta_0 - \\mathbf{x}_i^T \\boldsymbol{\\beta_i})$$\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial {\\beta_i}} = -\\frac{2}{N} \\sum_{i=1}^{N}(y_i - \\beta_0 + \\mathbf{x}_i^T \\boldsymbol{\\beta_i}) * x_i$$\n",
    "\n",
    "Now, let's set the parameters to 0 and add a learning rate (α) and number of times (i.e. epochs) to update each parameter.\n",
    "\n",
    "$\\alpha = 0.01$ will be the learning rate, and $t = 100$ will be the number of epochs.\n",
    "\n",
    "We want to update the model parameters using the following equations:\n",
    "\n",
    "$$\\beta_0^{t+1} = \\beta_0 + \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\beta_0}$$\n",
    "\n",
    "$$\\beta_i^{t+1} = \\beta_i + \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\beta_i}$$\n",
    "\n",
    "Substituting the derivatives we found earlier:\n",
    "\n",
    "$$\\beta_0^{t+1} = \\beta_0^{t+1} + 0.01 \\cdot (-\\frac{2}{N} \\sum_{i=1}^{N}(y_i - (\\beta_0^{t+1} + \\beta_i x_i)))$$\n",
    "\n",
    "$$\\beta_i^{t+1} = \\beta_i + 0.01 \\cdot (-\\frac{2}{N} \\sum_{i=1}^{N}(y_i - (\\boldsymbol{\\beta_0} + \\beta_i \\boldsymbol{x_i})) \\cdot \\boldsymbol{x_i})$$\n",
    "\n",
    "Simplifying the equations:\n",
    "\n",
    "$$\\beta_0^{t+1} = \\beta_0 - 0.02 \\sum_{i=1}^{N}(y_i - (\\boldsymbol{\\beta_0} + \\beta_i \\boldsymbol{x_i}))$$\n",
    "\n",
    "$$\\beta_i^{t+1} = \\beta_i - 0.02 \\sum_{i=1}^{N}(y_i - (\\boldsymbol{\\beta_0} + \\beta_i \\boldsymbol{x_i})) \\cdot \\boldsymbol{x_i}$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **0) How to Get Started**\n",
    "\n",
    "To perform Gradient Descent (GD), you must first decide on a cost function, also known as a loss or objective function.\n",
    "\n",
    "For multiple regression, we typically use the mean squared error (MSE) as the cost function:\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\| \\mathbf{y} - \\beta_0 \\mathbf{1} - \\mathbf{X} \\boldsymbol{\\beta} \\|^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*N* := Number (scalar) of data points\\\n",
    "*y* := Vector of dependent variables (N * 1)\\\n",
    "*β_0* := Scalar intercept term\\\n",
    "*1* := Vector of 1's (n * 1)\\\n",
    "*X* := Matrix of independent variables (M * N)\\\n",
    "*β* := Vector of parameters (coefficients to be estimated, N * 1)\n",
    "\n",
    "At first, you might think, \"How can I calculate the residual without first having values for the parameters?\"\n",
    "\n",
    "GD iteratively determines the parameters' values updating the parameters at each step based on the gradient of the cost function, which, in this case, is the MSE.\n",
    "\n",
    "You add the value of the partial derivative of the MSE with regard to the particular parameter:\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\beta_0} = -\\frac{2}{N} \\mathbf{1}^T (\\mathbf{y} - \\beta_0 \\mathbf{1} - \\mathbf{X} \\boldsymbol{\\beta})$$\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\boldsymbol{\\beta}} = -\\frac{2}{N} \\mathbf{X}^T (\\mathbf{y} - \\beta_0 \\mathbf{1} - \\mathbf{X} \\boldsymbol{\\beta})$$\n",
    "\n",
    "Now, let's set the parameters to 0 and add a learning rate (α) and number of times (i.e. epochs) to update each parameter.\n",
    "\n",
    "$α = 0.01$ will be the learning rate, and $t = 100$ will be the number of epochs.\n",
    "\n",
    "The updates for the parameters are:\n",
    "\n",
    "$$\\beta_0^{(t+1)} = \\beta_0^{(t)} - \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\beta_0}$$\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\boldsymbol{\\beta}}$$\n",
    "\n",
    "Substituting the derivatives into the update equations:\n",
    "\n",
    "$$\\beta_0^{(t+1)} = \\beta_0^{(t)} + \\frac{2 \\alpha}{N} \\mathbf{1}^T (\\mathbf{y} - \\beta_0^{(t)} \\mathbf{1} - \\mathbf{X} \\boldsymbol{\\beta}^{(t)})$$\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\frac{2 \\alpha}{N} \\mathbf{X}^T (\\mathbf{y} - \\beta_0^{(t)} \\mathbf{1} - \\mathbf{X} \\boldsymbol{\\beta}^{(t)})$$\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The expressions $\\beta_0^{(t+1)}$ and $\\boldsymbol{\\beta}^{(t+1)}$ would give us the intercept and vector of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Deriving Parameters Using OLS & GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Comparing OLS & GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
